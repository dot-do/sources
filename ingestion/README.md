# Data Ingestion Platform

Scalable data ingestion platform for collecting, processing, and storing data from multiple sources using Cloudflare's edge infrastructure.

## ğŸ¯ Overview

This platform ingests data from:
- **NPM Registry** - 2.5M+ packages (metadata, dependencies, downloads)
- **PyPI** - 500K+ Python packages
- **GitHub** - User profiles, repositories, and commits (email extraction)
- **CZDS** - TLD zone files (DNS records for all domains)
- **WHOIS** - Domain registration data (RDAP & legacy protocols)
- **TheOrg** - Organization structure and people data
- **Email Patterns** - Aggregated from Hunter.io, Clearbit, and GitHub

## ğŸ—ï¸ Architecture

### R2-First Storage Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Flow                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Source APIs     Workers        Queues       R2 Storage     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                              â”‚
â”‚  NPM API  â”€â”€â”€â”€â”€â–¶ NPM       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                   â”‚
â”‚                  Worker    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                   â”‚
â”‚                                          â–¶ NDJSON Files     â”‚
â”‚  PyPI API â”€â”€â”€â”€â”€â–¶ PyPI      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  (batched)       â”‚
â”‚                  Worker    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                   â”‚
â”‚                                          â–¶ Parquet Files    â”‚
â”‚  GitHub   â”€â”€â”€â”€â”€â–¶ GitHub    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  (via Pipelines) â”‚
â”‚  API            Worker      Queue        â–¶                  â”‚
â”‚                             Processor    â–¶ year=YYYY/       â”‚
â”‚  CZDS API â”€â”€â”€â”€â”€â–¶ CZDS      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  month=MM/       â”‚
â”‚                  Worker    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  day=DD/          â”‚
â”‚                                          â–¶                   â”‚
â”‚  WHOIS    â”€â”€â”€â”€â”€â–¶ WHOIS     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ R2 Data          â”‚
â”‚  RDAP           Worker     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Catalog          â”‚
â”‚                                          â–¶                   â”‚
â”‚  TheOrg   â”€â”€â”€â”€â”€â–¶ Scraper   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ R2 SQL           â”‚
â”‚  Website        Worker     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Queries          â”‚
â”‚                                                              â”‚
â”‚                  D1 Database                                 â”‚
â”‚                  (Metadata Only)                             â”‚
â”‚                  â”œâ”€ Ingestion Runs                           â”‚
â”‚                  â”œâ”€ Sync State                               â”‚
â”‚                  â””â”€ Failed Messages                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **Unlimited Scale**: R2 storage grows with your data (no 10GB limit)
- **Cost Effective**: ~$20/month for 750GB ($0.015/GB/month)
- **Time Partitioned**: Data organized by `year/month/day` for efficient queries
- **Semi-Structured**: Typed fields + flexible JSON "data" field
- **Queue-Based**: Async processing with automatic retries
- **Multi-Protocol**: HTTP, Cron, and Queue triggers

## ğŸ“¦ Workers

### Ingestion Workers (7 total)

| Worker | Purpose | Cron | Lines |
|--------|---------|------|-------|
| **npm-ingestion** | NPM package sync | 6 hours | 404 |
| **pypi-ingestion** | PyPI package sync | Daily | 434 |
| **email-pattern-worker** | Email discovery | On-demand | 463 |
| **github-ingestion** | GitHub data + emails | 4 hours | 739 |
| **czds-ingestion** | Zone files | Daily 2am | 638 |
| **whois-ingestion** | WHOIS lookups | Daily 3am | 635 |
| **theorg-scraper** | Org structure | Weekly Mon | 532 |
| **queue-processor-r2** | R2 batch writer | Queue | 559 |

**Total:** ~4,400 lines of production code

### Infrastructure

| Component | Purpose | Lines |
|-----------|---------|-------|
| **Dockerfile** | Container deployment | 87 |
| **docker-compose.yml** | Local dev environment | 70 |
| **proxy-rotation.ts** | Multi-provider proxy mgmt | 437 |
| **server.ts** | HTTP server for containers | 114 |

## ğŸš€ Quick Start

**15-minute setup:**

```bash
# 1. Clone and install
cd data-ingestion
pnpm install
wrangler login

# 2. Create infrastructure
wrangler d1 create data-ingestion
wrangler d1 execute data-ingestion --file=db/d1-minimal-schema.sql
wrangler r2 bucket create data-ingestion
wrangler kv:namespace create "DATA_CACHE"
wrangler queues create ingestion-queue

# 3. Update wrangler.toml files with IDs

# 4. Set secrets
wrangler secret put GITHUB_TOKEN

# 5. Deploy
wrangler deploy --config workers/queue-processor-r2.wrangler.toml
wrangler deploy --config workers/github-ingestion.wrangler.toml

# 6. Test
curl https://data-ingestion-github.YOUR_SUBDOMAIN.workers.dev/status
```

See [QUICKSTART.md](./QUICKSTART.md) for full guide.

## ğŸ“š Documentation

- **[QUICKSTART.md](./QUICKSTART.md)** - 15-minute setup guide
- **[DEPLOYMENT.md](./DEPLOYMENT.md)** - Complete production deployment (400+ lines)
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System design and data flow (600+ lines)
- **[db/r2-schemas.md](./db/r2-schemas.md)** - Parquet schema definitions

## ğŸ’¾ Data Storage

### R2 Bucket Structure

```
r2://data-ingestion/
â”œâ”€â”€ npm/
â”‚   â””â”€â”€ year=2025/month=10/day=03/
â”‚       â”œâ”€â”€ packages_00001.parquet
â”‚       â””â”€â”€ packages_00002.parquet
â”œâ”€â”€ pypi/
â”‚   â””â”€â”€ year=2025/month=10/day=03/
â”œâ”€â”€ github/
â”‚   â”œâ”€â”€ profiles/year=2025/...
â”‚   â”œâ”€â”€ repos/year=2025/...
â”‚   â””â”€â”€ commits/year=2025/...
â”œâ”€â”€ emails/year=2025/...
â”œâ”€â”€ orgs/year=2025/...
â”œâ”€â”€ domains/year=2025/...
â”œâ”€â”€ whois/year=2025/...
â””â”€â”€ zones/
    â”œâ”€â”€ com/2025-10-03.zone
    â”œâ”€â”€ net/2025-10-03.zone
    â””â”€â”€ org/2025-10-03.zone
```

### D1 Database (Minimal)

Only operational metadata:
- `ingestion_runs` - Track sync jobs
- `sync_state` - Last sync timestamps
- `failed_messages` - Error tracking

**Size:** <1GB (stays well under D1's 10GB limit)

## ğŸ” Querying Data

### R2 SQL Queries

```sql
-- NPM packages by downloads
SELECT name, version, downloads_total
FROM read_parquet('r2://data-ingestion/npm/**/*.parquet')
WHERE downloads_total > 1000000
ORDER BY downloads_total DESC
LIMIT 100;

-- GitHub emails by domain
SELECT author_email, COUNT(*) as commit_count
FROM read_parquet('r2://data-ingestion/github/commits/**/*.parquet')
WHERE author_email LIKE '%@example.com'
GROUP BY author_email;

-- Email patterns with high confidence
SELECT domain, pattern, confidence
FROM read_parquet('r2://data-ingestion/emails/**/*.parquet')
WHERE confidence > 0.8;

-- Domains with DNSSEC
SELECT domain, tld, nameservers
FROM read_parquet('r2://data-ingestion/domains/**/*.parquet')
WHERE dnssec_enabled = true;
```

## ğŸ’° Cost Breakdown

### Minimal Setup (~$7/month)

| Service | Usage | Cost |
|---------|-------|------|
| Workers Paid Plan | Base | $5.00 |
| R2 Storage | 50GB | $0.75 |
| Queues | 1M messages | $0.40 |
| D1 | <1GB | $0.00 |
| **Total** | | **$6.15** |

### Full Production (~$320/month)

| Service | Usage | Cost |
|---------|-------|------|
| Cloudflare Workers | Base | $5.00 |
| R2 Storage | 750GB | $11.25 |
| R2 Operations | 100M reads | $0.40 |
| Queues | 10M messages | $4.00 |
| Container (optional) | 1 instance | $10.00 |
| WhoisXML API | 10k queries | $50.00 |
| Hunter.io | Basic | $49.00 |
| Proxy Service | Residential | $200.00 |
| **Total** | | **$329.65** |

## ğŸ› ï¸ Development

### Local Development

```bash
# Install dependencies
pnpm install

# Run type checking
pnpm typecheck

# Run tests
pnpm test

# Local development with Docker
docker-compose up

# Watch worker logs
wrangler tail data-ingestion-queue-processor
```

### Project Structure

```
data-ingestion/
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ d1-minimal-schema.sql       # D1 operational metadata
â”‚   â””â”€â”€ r2-schemas.md                # Parquet schemas
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.ts                    # HTTP server for containers
â”‚   â”œâ”€â”€ proxy-rotation.ts            # Proxy management
â”‚   â””â”€â”€ proxy-usage-example.ts       # Usage patterns
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ npm-ingestion.ts             # NPM worker
â”‚   â”œâ”€â”€ pypi-ingestion.ts            # PyPI worker
â”‚   â”œâ”€â”€ email-pattern-worker.ts      # Email worker
â”‚   â”œâ”€â”€ github-ingestion.ts          # GitHub worker
â”‚   â”œâ”€â”€ czds-ingestion.ts            # CZDS worker
â”‚   â”œâ”€â”€ whois-ingestion.ts           # WHOIS worker
â”‚   â”œâ”€â”€ theorg-scraper.ts            # TheOrg worker
â”‚   â””â”€â”€ queue-processor-r2.ts        # Queue consumer
â”œâ”€â”€ Dockerfile                       # Container image
â”œâ”€â”€ docker-compose.yml               # Local dev stack
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ wrangler.toml                    # Root config
â””â”€â”€ workers/*.wrangler.toml          # Per-worker configs
```

## ğŸ” Security

### Required Secrets

```bash
# GitHub API (required for GitHub worker)
wrangler secret put GITHUB_TOKEN

# CZDS (required for zone files)
wrangler secret put CZDS_USERNAME
wrangler secret put CZDS_PASSWORD

# Optional API keys
wrangler secret put WHOISXML_API_KEY
wrangler secret put HUNTER_API_KEY
wrangler secret put CLEARBIT_API_KEY

# Optional proxy credentials
wrangler secret put PROXY_USER
wrangler secret put PROXY_PASS
```

### Best Practices

- âœ… All credentials stored as Worker secrets
- âœ… Rate limiting per API provider
- âœ… Proxy rotation for scraping
- âœ… GDPR-compliant data handling
- âœ… Error tracking and alerting
- âœ… Cost monitoring and alerts

## ğŸ“Š Monitoring

### Health Checks

```bash
# Check worker status
curl https://data-ingestion-{worker}.{subdomain}.workers.dev/status

# View recent runs
wrangler d1 execute data-ingestion --command="
  SELECT * FROM ingestion_runs
  ORDER BY started_at DESC
  LIMIT 10;
"

# Check queue depth
wrangler queues list

# Monitor R2 storage
wrangler r2 object list data-ingestion | wc -l
```

### Metrics

- Ingestion runs (success/failure rates)
- Queue message counts
- R2 storage size and growth
- Worker execution time
- API rate limit usage

## ğŸ› Troubleshooting

### Common Issues

**Workers not deploying?**
```bash
wrangler --version  # Must be latest
npm install -g wrangler@latest
```

**Queue backing up?**
- Increase `max_batch_size` in wrangler.toml
- Scale up queue consumer concurrency

**High R2 costs?**
- Implement data retention policies
- Delete old partitions: `r2 object delete --prefix=npm/year=2024/`

**Rate limited by APIs?**
- Enable proxy rotation
- Reduce cron frequency
- Add delays between requests

See [DEPLOYMENT.md](./DEPLOYMENT.md) for detailed troubleshooting.

## ğŸ”„ Data Pipeline

1. **Ingestion Workers** fetch data from source APIs
2. **Messages** queued to `ingestion-queue`
3. **Queue Processor** batches messages
4. **NDJSON** written to R2 (streaming-friendly format)
5. **Pipelines** convert NDJSON â†’ Parquet (automatic)
6. **Data Catalog** indexes partitions
7. **R2 SQL** provides query interface

## ğŸ¯ Use Cases

- **Package Intelligence** - Track NPM/PyPI ecosystem
- **Email Discovery** - Find contact patterns at scale
- **Domain Analytics** - DNS and WHOIS data analysis
- **Organization Mapping** - Company structure insights
- **Developer Intelligence** - GitHub activity analysis
- **Security Research** - Domain and DNSSEC monitoring

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please open issues or pull requests.

---

**Status:** Production Ready
**Setup Time:** 15 minutes
**Monthly Cost:** $7-$320 (depending on usage)
**Scale:** Unlimited (R2-first architecture)

Built with â¤ï¸ using Cloudflare Workers, R2, D1, and Queues
